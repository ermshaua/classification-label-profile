# CLaP - State Detection from Time Series

This is the supporting website for the paper "CLaP - State Detection from Time Series" (under review). It contains the used source codes, the data sets, raw results, and analysis notebooks. It reflects the state of the paper for reproducibility and is purposely not further updated.


The ever-growing amount of sensor data from machines, smart devices, and the environment leads to an abundance of high-resolution, unannotated time series (TS). These recordings encode recognizable properties of latent states and transitions from physical phenomena that can be modelled as abstract processes. The unsupervised localization and identification of these states and their transitions is the task of time series state detection (TSSD). Current TSSD algorithms employ classical unsupervised learning techniques, to infer state membership directly from feature space. This limits their predictive power, compared to supervised learning methods, which can exploit additional label information. We introduce CLaP, a new, highly accurate and efficient algorithm for TSSD. It leverages the predictive power of time series classification for TSSD in an unsupervised setting by applying novel self-supervision techniques to detect whether data segments emerge from the same state. To this end, CLaP cross-validates a classifier with segment-labelled subsequences to quantify confusion between segments. It merges labels from segments with high confusion, representing the same latent state, if this leads to an increase in overall classification quality. We conducted an experimental evaluation using 405 TS from five benchmarks and found CLaP to be significantly more precise in detecting states than six state-of-the-art competitors. It achieves the best accuracy-runtime tradeoff and is scalable to large TS. We provide a Python implementation of CLaP, which can be deployed in TS analysis workflows.


## Benchmark Results

We have evaluated CLaP and six competitors on five benchmark data sets with 405 time series. The following table summarises the average Covering / Adjusted Mutual Information (AMI) performance (higher is better) and the corresponding wins & ties. More details are in the paper. The raw measurements are <a target="_blank" href="https://github.com/ermshaua/classification-label-profile/blob/main/experiments">here</a> and analysis Jupyter notebooks are <a target="_blank" href="https://github.com/ermshaua/classification-label-profile/blob/main/notebooks/comparative_analysis/">here</a>.

| State Detection Algorithm | Average (in %) | Std. Dev. (in %) | Median (in %) | Wins & Ties (in %) |
|---------------------------|---------------|------------------|---------------|--------------------|
| CLaP                      | 72.9 / 60.4   | 21.6 / 30.9      | 76.0 / 67.0   | 50.4 / 47.7            |
| ClaSP2Feat                | 68.4 / 44.1   | 22.9 / 34.4      | 68.1 / 43.9   | 36.0 / 23.2            |
| TICC                      | 57.2 / 24.4   | 24.2 / 34.8      | 53.4 / 3.1    | 21.2 / 15.3            |
| E2USD                     | 50.0 / 42.0   | 23.3 / 27.3      | 49.1 / 45.4   | 13.8 / 10.9            |
| Time2State                | 43.9 / 44.3   | 22.6 / 23.3      | 40.7 / 46.3   | 5.7 / 7.9             |
| AutoPlait                 | 45.8 / 17.5   | 25.0 / 32.9      | 39.6 / 0.0    | 14.8 / 11.1            |
| HDP-HSMM                  | 38.7 / 41.5   | 24.1 / 27.9      | 35.7 / 43.9   | 9.1 / 17.3             |


## Organisation

This repository is structured in the following way: 

- `benchmark` contains the source codes used for running the paper experiments.
- `datasets` consists of the five benchmark data sets (TSSB, UTSA, HAS, SKAB, MIT-BIH).
- `experiments` contains the raw measurement results for CLaP and the six competitors. 
- `external` contains the sources codes of the competitors and external utility methods.
- `figures` includes the paper plots, generated by the Jupyter notebooks.
- `notebooks` consists of Jupyter notebooks, used to analyse data sets and results.
- `src` contains the sources codes of CLaP and utility methods.

## Installation

You can download this repository (by clicking the download button in the upper right corner). As this repository is a supporting website and not an updated library, we do not recommend to install it! Extract or adapt code snippets of interest. We are currently working on integrating CLaP as a part of the maintained and updated <a href="https://github.com/ermshaua/claspy" target="_blank">claspy</a> library.

## Citation

The associated paper is currently under review. Thereafter, you will find the citation request here.

## Resources

The sources codes for the competitors in the benchmark evaluation come from multiple authors and projects. We list here the resources we used (and adapted) for our experiments:
- Evaluation Metrics (https://github.com/alan-turing-institute/TCPDBench)
- AutoPlait (https://sites.google.com/site/onlinesemanticsegmentation/)
- E2USD (https://github.com/AI4CTS/E2Usd)
- Time2State & HDP-HSMM (https://github.com/Lab-ANT/Time2State/)
- TICC (https://github.com/davidhallac/TICC)
- Time2Feat (https://github.com/softlab-unimore/time2feat)
